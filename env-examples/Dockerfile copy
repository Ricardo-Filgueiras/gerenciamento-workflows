FROM apache/airflow:2.10.4 
# Nota: Verifique a versão compatível, usei a 2.10 como exemplo estável, 
# mas pode manter a 3.x se estiver seguro da estabilidade.

USER root

# 1. Instalar OpenJDK-17 (Necessário para o Spark) e utilitários
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
         openjdk-17-jre-headless \
         procps \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

# 2. Configurar JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

# 3. Instalar bibliotecas Python para Spark e MinIO
USER airflow
RUN pip install --no-cache-dir \
    apache-airflow-providers-apache-spark \
    pyspark==3.5.0 \
    delta-spark==3.0.0 \
    s3fs

# Opcional: Se você quiser os binários completos do Spark dentro do Airflow
# para submissão local (client mode), teria que baixar e descompactar o TGZ do Spark aqui.
# Mas instalando o 'pyspark' via pip, ele já traz o binário básico do spark-submit em muitos casos.